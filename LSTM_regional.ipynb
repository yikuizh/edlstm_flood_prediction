{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_regional.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-MPua5eBFUk"
      },
      "source": [
        "'''\n",
        "Load the service of Google Colab -- use Colab GPU for training\n",
        "\n",
        "'''\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8fF0Kl-pm8v"
      },
      "source": [
        "'''\n",
        "Link to Google Drive root\n",
        "'''\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Import required packages\n",
        "'''\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers import TimeDistributed, Dense, LSTM, RepeatVector, Dropout, Input, Flatten,Concatenate\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from tensorflow import optimizers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import glob,os\n",
        "import random\n",
        "import os,shutil"
      ],
      "metadata": {
        "id": "KwzDHOL821Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNZrjdFkfpH8"
      },
      "source": [
        "def listpath(path, list_name):  \n",
        "       \"\"\"List name of all csv files in a folder\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : \n",
        "            Path of root folder\n",
        "        Returns\n",
        "        -------\n",
        "        list\n",
        "            The name list of csv file in the path\n",
        "        \"\"\"\n",
        "    for file in os.listdir(path):\n",
        "        file_path = os.path.join(path, file)\n",
        "        if os.path.isdir(file_path):\n",
        "            listdir(file_path, list_name)\n",
        "        else:\n",
        "            list_name.append(file_path)\n",
        "        return list_name\n",
        "\n",
        "def custom_regional_nse(y_true, y_pred):\n",
        "    \"\"\"Cusmomized batch-wise loss function\n",
        "        Parameters\n",
        "        ---------\n",
        "        y_true:\n",
        "            Ground truth observation\n",
        "        y_pred:\n",
        "            Output from batch-wise prediction\n",
        "        Returns\n",
        "        --------\n",
        "        Normalized batch-wise loss\n",
        "\n",
        "    \"\"\"\n",
        "    numerator = K.square(y_pred - y_true)  # (batch_size, 2)\n",
        "    dominator = K.square(y_true - K.mean(y_true)) + 0.1\n",
        "    # summing both loss values along batch dimension \n",
        "    numerator_sum = K.sum(numerator, axis=1)        # (batch_size,)\n",
        "    dominator_sum = K.sum(dominator, axis=1)        # (batch_size,)\n",
        "    loss = (numerator_sum)/(dominator_sum)\n",
        "    return loss # automatical average along batch size when optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2QgZXMfuWpb"
      },
      "source": [
        "def split_input(static, dataset, n_input, n_output, scaler, shuffle): # \n",
        "        \"\"\"Prepare structured and paired in/output sequence for LSTM model\n",
        "        Parameters\n",
        "        ---------\n",
        "        static:\n",
        "            14 catchment-wise static variables\n",
        "        dataset:\n",
        "            The name list of csv file in the path\n",
        "        n_input:\n",
        "            length of input sequence (i.e. 24 hours)\n",
        "        n_output:\n",
        "            length of output sequence (i.e. 6 hours)\n",
        "        scaler:\n",
        "            pre-defined Min-Max scaler for input data\n",
        "        shuffle: Boolean\n",
        "            True or False\n",
        "        Returns\n",
        "        --------\n",
        "        x1: np.array\n",
        "            runoff observation data, paired with x2, y, static_new for each prediction\n",
        "        x2:\n",
        "            rainfall, tmax, tmin observation data (input + output length)\n",
        "        y:\n",
        "            target runoff data\n",
        "        static_new:\n",
        "            matrix of static variables (14, input + output length)\n",
        "    \"\"\"\n",
        "    x1, x2, y, seq_static = list(),list(), list(), list()\n",
        "    for j in range(len(dataset)): # go through each catchment\n",
        "        data_1 = pd.read_csv(dataset[j], index_col = 0) \n",
        "        data_2 = pd.read_csv(dataset[j], index_col = 0) # read again for non-scalered target\n",
        "        \n",
        "        sequence = scaler.transform(data_1) # introduce MinMax scaler for individual event, only for input data\n",
        "        cat = dataset[j].split('-')[3]\n",
        "        catchment = cat.split('.')[0] # extract the catchment name\n",
        "        temp_static = np.zeros((1,14))\n",
        "        temp_static = static[catchment].values # each basin one static input only one step\n",
        "\n",
        "        for i in range(len(sequence)): # go through each event\n",
        "            end_ix1 = i + n_input # runoff observation data\n",
        "            end_ix2 = i + n_input + n_output # rainfall, tmax, tmin observation data (input + output length)\n",
        "            end_y = end_ix1 + n_output\n",
        "            # check if we are beyond the sequence\n",
        "            if end_ix1 > len(sequence):\n",
        "                break\n",
        "            if end_ix2 > len(sequence):\n",
        "                break\n",
        "            if end_y > len(sequence):\n",
        "                break\n",
        "            seq_x1, seq_y = sequence[:,0][i:end_ix1], data_2.values[:,0][end_ix1:end_y] # input runoff observation and target runoff \n",
        "            seq_x2 = sequence[:,1:][i:end_ix2, :] # input rainfall, tmax and tmin observation\n",
        "            x1.append(seq_x1)\n",
        "            x2.append(seq_x2)\n",
        "            y.append(seq_y)\n",
        "            seq_static.append(list(temp_static))\n",
        "\n",
        "    x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n",
        "    number_list = np.arange(0,len(x1),1)\n",
        "    random.shuffle(number_list) \n",
        "    x1_new = []\n",
        "    for index in number_list:\n",
        "        x1_new.append(x1[index])\n",
        "    x2_new = []\n",
        "    for index in number_list:\n",
        "        x2_new.append(x2[index])\n",
        "    y_new = []\n",
        "    for index in number_list:\n",
        "        y_new.append(y[index])\n",
        "    static_new = []\n",
        "    for index in number_list:\n",
        "        static_new.append(seq_static[index])\n",
        "    #  only shuffle the sliced in/output sequence when training model\n",
        "    if shuffle:\n",
        "        return np.array(x1_new), np.array(x2_new), np.array(y_new), np.array(static_new)\n",
        "    else:\n",
        "        return x1, x2, y, static_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10rgD9qUuXDc"
      },
      "source": [
        "nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name.csv',index_col = 0)\n",
        "basins = nse_table.columns # collect name of each basin\n",
        "\n",
        "# read 14 catchment static variables (PC scores)\n",
        "pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n",
        "temp2 = pca_score.values\n",
        "temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) \n",
        "pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf6VIhp2F48p"
      },
      "source": [
        "\"\"\"\n",
        "When training a regional model, flood events at each catchment are preliminaryly assigned portionly (50/25/25) into three folders\n",
        "-- train1, val, and test for a convenient use\n",
        "\"\"\"\n",
        "\n",
        "train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/train1/'\n",
        "val_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/val/'\n",
        "test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/test/'\n",
        "\n",
        "trainlist=os.listdir(train_path)\n",
        "vallist=os.listdir(val_path)\n",
        "\n",
        "# Create scaler\n",
        "df2 = pd.DataFrame()\n",
        "for j in range(len(trainlist)):\n",
        "  for basin in basins:\n",
        "    if trainlist[j].find(basin) != -1:\n",
        "      data = pd.read_csv(train_path + trainlist[j], index_col = 0)\n",
        "      df2 = pd.concat([df2,data],axis=0) \n",
        "temp = df2.values\n",
        "min_max_scaler1 = MinMaxScaler() \n",
        "min_max_scaler1.fit(temp) \n",
        "\n",
        "# input for training model\n",
        "train1_x, train2_x, train_y, train_static = split_input(pca_score_nor,train_path, trainlist, 24, 6,min_max_scaler1,shuffle =True)\n",
        "# input for validation model\n",
        "val1_x, val2_x, val_y, val_static = split_input(pca_score_nor,val_path, vallist, 24, 6, min_max_scaler1,shuffle =True)\n",
        "\n",
        "train1_x = train1_x.reshape((train1_x.shape[0],train1_x.shape[1],1))\n",
        "val1_x = val1_x.reshape((val1_x.shape[0],val1_x.shape[1],1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY47wyUmuni0"
      },
      "source": [
        "\"\"\"\n",
        "LSTM model structure and training; the hyperparameter selection has been skipped here (pre-trained model)\n",
        "\"\"\"\n",
        "\n",
        "######## design LSTM network structure ########\n",
        "dim_dense=[512, 256, 256, 128, 64]\n",
        "# define customer optimizer\n",
        "RMSprop= optimizers.RMSprop(lr=0.0001)\n",
        "# define special callbacks\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n",
        "# define early stop\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=2)\n",
        "\n",
        "# input of runoff observation, LSTM encoder\n",
        "input_1 = Input(shape=(train1_x.shape[1],train1_x.shape[2]), name='LSTM1_input') # shape should be 72*1 for runoff observation\n",
        "LSTM1 = LSTM(256, return_sequences=False)(input_1)\n",
        "\n",
        "# input of runoff observation and forecast, LSTM encoder\n",
        "input_2 = Input(shape=(train2_x.shape[1],train2_x.shape[2]), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for n stations (if there is no upstream station, n=1)\n",
        "LSTM2 = LSTM(256, return_sequences=False)(input_2)\n",
        "\n",
        "# input of other non-timeseries data, such as daily or monthly data.\n",
        "input_static = Input(shape=(14,), name='static_input') # shape = 14 -- 14 static variables -- 2D in this case with only Input or Dense layer (14,)/3D for LSTM layer (1,14)\n",
        "Layer3 = Dense(16, activation='relu')(input_static)\n",
        "\n",
        "# connect all data\n",
        "x = Concatenate()([Layer3, LSTM1, LSTM2]) \n",
        "x = RepeatVector(6)(x)  # 6 hour ahead prediction\n",
        "\n",
        "# LSTM decoder\n",
        "x = LSTM(512, return_sequences=True)(x)\n",
        "\n",
        "# final fully-connected layer for final result\n",
        "for dim in dim_dense:\n",
        "\tx = TimeDistributed(Dense(dim, activation='relu'))(x)\n",
        "\tx = TimeDistributed(Dropout(0.4))(x) # Some dropout for dense layers.\n",
        "main_out = TimeDistributed(Dense(1, activation='relu'))(x) \n",
        "main_out = Flatten()(main_out)\n",
        "\n",
        "################## model training ####################\n",
        "model = Model(inputs=[input_static, input_1, input_2], outputs=main_out)\n",
        "model.compile(optimizer=RMSprop, loss=custom_regional_nse) \n",
        "model.summary()\n",
        "\n",
        "# fit network \n",
        "history = model.fit([train_static, train1_x, train2_x], train_y, epochs=300, batch_size=128,\n",
        "\t\t\t\t\tvalidation_data=([val_static, val1_x, val2_x], val_y), callbacks=[reduce_lr,early_stopping], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6afFA3futU8"
      },
      "source": [
        "# visualize the train/val loss\n",
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss']) \n",
        "plt.title( 'model train vs validation loss') \n",
        "plt.ylabel('loss') \n",
        "plt.xlabel('epoch') \n",
        "plt.legend([ 'train', 'validation'], loc= 'upper right') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/master_thesis/_code/regional_model/regional.h5' ) "
      ],
      "metadata": {
        "id": "JNhrTHgf6wOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### model evaluation ######\n",
        "model_regional = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/regional.h5' % (num_ite), \n",
        "                           custom_objects={'custom_regional_nse': custom_regional_nse})\n",
        "\n",
        "nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name.csv',index_col = 0)\n",
        "basins = nse_table.columns\n",
        "\n",
        "for catchment in basins:\n",
        "\n",
        "    path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events/' % catchment\n",
        "    file_all = sorted(os.listdir(path))\n",
        "    split2 = int(len(file_all)*0.75)  \n",
        "    dataset = file_all[split2:] \n",
        "    ed_nse = []\n",
        "    ed_hour = []\n",
        "    ed_volume = []\n",
        "    ed_peak = []\n",
        "    dates = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        # pre-processing the input sequence for performance evaluation -- event-wise\n",
        "            x1, x2,y, seq_static = list(),list(), list(), list()\n",
        "            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n",
        "            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n",
        "            date = dataset[j][:10]\n",
        "\n",
        "            # MinMax for individual event\n",
        "            n_input = 24\n",
        "            n_output = 6\n",
        "            sequence = min_max_scaler1.transform(data_1) # same scaler as training dataset\n",
        "\n",
        "            # add static variable\n",
        "            temp_static = np.zeros((1,14))\n",
        "            temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n",
        "            for i in range(len(sequence)):\n",
        "                # find the end of this pattern\n",
        "                end_ix1 = i*n_output + n_input # runoff\n",
        "                end_ix2 = i*n_output + n_input + n_output # others\n",
        "                end_y = end_ix1 + n_output\n",
        "                # check if we are beyond the sequence\n",
        "                if end_ix1 > len(sequence):\n",
        "                    break\n",
        "                if end_ix2 > len(sequence):\n",
        "                    break\n",
        "                if end_y > len(sequence):\n",
        "                    break\n",
        "                seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] \n",
        "                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] \n",
        "                x1.append(seq_x1)\n",
        "                x2.append(seq_x2)\n",
        "                y.append(seq_y)\n",
        "                seq_static.append(list(temp_static))\n",
        "\n",
        "            #event-wise model performance evaluation\n",
        "            x1, x2, y, seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n",
        "            predictions = []\n",
        "            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n",
        "\n",
        "            predict = model_regional.predict([seq_static, x1,x2]) \n",
        "            predictions.append(predict)\n",
        "            dates.append(date) \n",
        "            ed_peak.append(np.max(np.array(predictions).flatten()))\n",
        "            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n",
        "\n",
        "            # peak hour\n",
        "            hour_pre = np.argmax(np.array(predictions).flatten())\n",
        "            hour_obs = np.argmax(np.array(y).flatten())\n",
        "            ed_hour.append((hour_pre - hour_obs))\n",
        "            \n",
        "            # peak discharge\n",
        "            max_pre = np.max(np.array(predictions).flatten())\n",
        "            max_obs = np.max(np.array(y).flatten())\n",
        "            ed_volume.append((max_pre - max_obs)/max_obs)\n",
        "\n",
        "        temp = np.array([dates, ed_peak, ed_hour, ed_volume]) # statistics for event-wise prediction at each catchment\n",
        "        output = pd.DataFrame([temp[0,:],temp[1,:],temp[2,:],temp[3,:]], index=['date', 'flow', 'hour','ratio'])\n",
        "        output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub/%s_peak_pub.csv' % (catchment))\n",
        "        nse_table[catchment] = np.median(ed_nse) # collect NSE score for each basin in one test fold"
      ],
      "metadata": {
        "id": "6dVkoLh836MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/regional_nse.csv')"
      ],
      "metadata": {
        "id": "C_J20kXA9Y5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}