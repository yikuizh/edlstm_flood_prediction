{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pub_nostatic.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4mt7ENmEyam","executionInfo":{"status":"ok","timestamp":1617209526085,"user_tz":-120,"elapsed":229774,"user":{"displayName":"Yikui Zhang","photoUrl":"","userId":"16649669106471700390"}},"outputId":"59050a1e-5888-4ae0-e6be-0913b53bc60e"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 160980 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.24-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgRDrA9lFAYp","executionInfo":{"status":"ok","timestamp":1617209567671,"user_tz":-120,"elapsed":15342,"user":{"displayName":"Yikui Zhang","photoUrl":"","userId":"16649669106471700390"}},"outputId":"dd94679d-a53b-4345-b050-5d52a2db424d"},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1hEb_qKYOMwiS99gtfcuhfmhusLrh_QLJ/master_thesis/_code/regional_model/regional_data/model1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYWFU_naU1GU"},"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn import preprocessing\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import mean_squared_error\n","from keras.layers import TimeDistributed, Dense, LSTM, RepeatVector, Dropout, Input, Flatten,Concatenate\n","import numpy as np\n","import pandas as pd\n","import keras\n","import matplotlib.pyplot as plt\n","import glob,os\n","import random\n","from keras.models import Model\n","import os,shutil\n","# read model\n","from keras.models import load_model\n","from sklearn.metrics import r2_score \n","\n","def listpath(path, list_name):  # 传入存储的list\n","    for file in os.listdir(path):\n","        file_path = os.path.join(path, file)\n","        if os.path.isdir(file_path):\n","            listdir(file_path, list_name)\n","        else:\n","            list_name.append(file_path)\n","        return list_name\n","\n","\n","import keras.backend as K\n","def custom_regional_nse(y_true, y_pred):\n","\n","    numerator = K.square(y_pred - y_true)  # (batch_size, 2)\n","    dominator = K.square(y_true - K.mean(y_true)) + 0.1\n","    # summing both loss values along batch dimension \n","    numerator_sum = K.sum(numerator, axis=1)        # (batch_size,)\n","    dominator_sum = K.sum(dominator, axis=1)        # (batch_size,)\n","    loss = (numerator_sum)/(dominator_sum)\n","    return loss # automatical average along batch size when optimization\n","\n","def split_pub(static, dataset, n_input, n_output, scaler,shuffle): # \n","    x1, x2,y = list(),list(), list()\n","    for j in range(len(dataset)):\n","        data_1 = pd.read_csv(dataset[j], index_col = 0)\n","        data_2 = pd.read_csv(dataset[j], index_col = 0) # not normalize y(target)\n","        # MinMax for individual event\n","        sequence = scaler.transform(data_1)\n","        cat = dataset[j].split('-')[3]\n","        catchment = cat.split('.')[0]\n","        temp_static = np.zeros(( n_input + n_output,14))\n","        for i in range(temp_static.shape[1]):\n","            temp_static[i,:] = static[catchment].values\n","        for i in range(len(sequence)):\n","            # find the end of this pattern\n","            end_ix1 = i + n_input # runoff\n","            end_ix2 = i + n_input + n_output # others\n","            end_y = end_ix1 + n_output\n","            # check if we are beyond the sequence\n","            if end_ix1 > len(sequence):\n","                break\n","            if end_ix2 > len(sequence):\n","                break\n","            if end_y > len(sequence):\n","                break\n","            # gather input and output parts of the pattern\n","            temp_x2 = sequence[:,1:][i:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","            seq_x1, seq_y = sequence[:,0][i:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","            # concatnate temp_static and seq_x2\n","\n","            seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","            x1.append(seq_x1)\n","            x2.append(seq_x2)\n","            y.append(seq_y)\n","\n","#shuffle\n","    x1, x2,y = np.array(x1), np.array(x2), np.array(y)\n","    number_list = np.arange(0,len(x1),1)\n","    random.shuffle(number_list) \n","    x1_new = []\n","    for index in number_list:\n","        x1_new.append(x1[index])\n","    x2_new = []\n","    for index in number_list:\n","        x2_new.append(x2[index])\n","    y_new = []\n","    for index in number_list:\n","        y_new.append(y[index])\n","    if shuffle:\n","        return np.array(x1_new), np.array(x2_new), np.array(y_new)\n","    else:\n","        return x1,x2, y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_BQL1WPVKZC"},"source":["# modified: only normalize the input instead of x and y\n","### RM3 version\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn import preprocessing\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import mean_squared_error\n","from keras.layers import TimeDistributed, Dense, LSTM, RepeatVector, Dropout, Input, Flatten,Concatenate\n","import numpy as np\n","import pandas as pd\n","import keras\n","import matplotlib.pyplot as plt\n","import glob,os\n","import random\n","from keras.models import Model\n","import os,shutil\n","# read model\n","from keras.models import load_model\n","from sklearn.metrics import r2_score \n","\n","def listpath(path, list_name):  # 传入存储的list\n","    for file in os.listdir(path):\n","        file_path = os.path.join(path, file)\n","        if os.path.isdir(file_path):\n","            listdir(file_path, list_name)\n","        else:\n","            list_name.append(file_path)\n","        return list_name\n","\n","\n","import keras.backend as K\n","def custom_regional_nse(y_true, y_pred):\n","\n","    numerator = K.square(y_pred - y_true)  # (batch_size, 2)\n","    dominator = K.square(y_true - K.mean(y_true)) + 0.1\n","    # summing both loss values along batch dimension \n","    numerator_sum = K.sum(numerator, axis=1)        # (batch_size,)\n","    dominator_sum = K.sum(dominator, axis=1)        # (batch_size,)\n","    loss = (numerator_sum)/(dominator_sum)\n","    return loss # automatical average along batch size when optimization\n","\n","\n","def split_pub(static, dataset, n_input, n_output, scaler,shuffle): # \n","    x1, x2,y,seq_static = list(),list(), list(), list()\n","    for j in range(len(dataset)):\n","        data_1 = pd.read_csv(dataset[j], index_col = 0)\n","        data_2 = pd.read_csv(dataset[j], index_col = 0) # not normalize y(target)\n","        # MinMax for individual event\n","        sequence = scaler.transform(data_1)\n","        cat = dataset[j].split('-')[3]\n","        catchment = cat.split('.')[0]\n","        \n","        temp_static = np.zeros((1,14))\n","        temp_static = static[catchment].values # each basin one static input only one step\n","        for i in range(len(sequence)):\n","            # find the end of this pattern\n","            end_ix1 = i + n_input # runoff\n","            end_ix2 = i + n_input + n_output # others\n","            end_y = end_ix1 + n_output\n","            # check if we are beyond the sequence\n","            if end_ix1 > len(sequence):\n","                break\n","            if end_ix2 > len(sequence):\n","                break\n","            if end_y > len(sequence):\n","                break\n","            # gather input and output parts of the pattern\n","            seq_x2 = sequence[:,1:][i:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","            seq_x1, seq_y = sequence[:,0][i:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","            # concatnate temp_static and seq_x2\n","            # seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","            x1.append(seq_x1)\n","            x2.append(seq_x2)\n","            y.append(seq_y)\n","            seq_static.append(list(temp_static))\n","\n","\n","    #shuffle\n","    x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","    number_list = np.arange(0,len(x1),1)\n","    random.shuffle(number_list) \n","    x1_new = []\n","    for index in number_list:\n","        x1_new.append(x1[index])\n","    x2_new = []\n","    for index in number_list:\n","        x2_new.append(x2[index])\n","    y_new = []\n","    for index in number_list:\n","        y_new.append(y[index])\n","    static_new = []\n","    for index in number_list:\n","        static_new.append(seq_static[index])\n","    if shuffle:\n","        return np.array(x1_new), np.array(x2_new), np.array(y_new), np.array(static_new)\n","    else:\n","        return x1,x2, y, static_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWFjmU-kVlWw"},"source":["nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=10, shuffle = True,random_state = 1) # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in kf.split(basins):\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cf2NuYbFzXRV"},"source":["nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=5, shuffle = True,random_state = 1) # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in kf.split(basins):\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yr8yPrupBpHC"},"source":["test_index_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nuct9bh2UzTZ"},"source":["test_index_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeFlxD9Kpbe1"},"source":["# leave one out\n","nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import LeaveOneOut # leave-one out kfolder\n","loo = LeaveOneOut() # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in loo.split(basins):\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 0) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mmq-pZ7yD9Lu"},"source":["test_index_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZmr8rQZWZtN"},"source":["######################## first folder ##########################\n","for k in range(len(train_index_list)):\n","#for k in range(6,8):\n","    num_ite = k + 1 # count the number of folder\n","    print (num_ite)\n","\n","    train_sub_index = train_index_list[k] # try the first folder\n","    train_file_all = []\n","    for i in range(len(train_sub_index)):\n","        catchment = basins[train_sub_index[i]]\n","        train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        trainlist = os.listdir(train_path)\n","        temp_name = []\n","        for i in range(len(trainlist)):\n","          temp = train_path + '/'+ trainlist[i]\n","          train_file_all.append(temp)\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","    for i in range(len(test_sub_index)):\n","        catchment = basins[test_sub_index[i]]\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        temp_name = []\n","        for i in range(len(testlist)):\n","          temp = test_path + '/'+ testlist[i]\n","          test_file_all.append(temp)\n","\n","\n","    # dataset split -- train_file_all\n","    # all data should use this scaler--fit the scaler using all data\n","    df2 = pd.DataFrame()\n","    for j in range(len(train_file_all)):\n","        data = pd.read_csv(train_file_all[j], index_col = 0)\n","        df2 = pd.concat([df2,data],axis=0) \n","    temp = df2.values\n","    min_max_scaler1 = MinMaxScaler() # default for each column\n","    min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","    # input for training model\n","    train1_x, train2_x, train_y = split_pub(pca_score_nor, train_file_all, 24, 6,min_max_scaler1,shuffle =True)\n","    #train1_x, train2_x, train_y = split_multi_sequence_keras(train_val, 24, 6,min_max_scaler1,shuffle =True)\n","\n","    # input for validation model -- using test\n","    val1_x, val2_x, val_y = split_pub(pca_score_nor, test_file_all, 24, 6, min_max_scaler1,shuffle =True)\n","    #val1_x, val2_x, val_y = split_multi_sequence_keras(test, 24, 6, min_max_scaler1,shuffle =True)\n","\n","    val1_x = val1_x.reshape((val1_x.shape[0],val1_x.shape[1],1)) \n","    train1_x = train1_x.reshape((train1_x.shape[0],train1_x.shape[1],1))\n","\n","    # define customer optimizer\n","    RMSprop=keras.optimizers.RMSprop(lr=0.0001)\n","\n","    # define special callbacks\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n","    ######## early longer ########\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=2)\n","    # early stop 50\n","    # design network\n","    dim_dense=[512, 256, 256, 128, 64]\n","\n","    # input of runoff observation, LSTM encoder\n","    input_1 = Input(shape=(train1_x.shape[1],train1_x.shape[2]), name='LSTM1_input') # shape should be 72*1 for runoff observation\n","    LSTM1 = LSTM(256, return_sequences=False)(input_1)\n","\n","    # input of runoff observation and forecast, LSTM encoder\n","    input_2 = Input(shape=(train2_x.shape[1],train2_x.shape[2]), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for n stations (if there is no upstream station, n=1)\n","    LSTM2 = LSTM(256, return_sequences=False)(input_2)\n","\n","\n","    # connect all data\n","    x = Concatenate()([LSTM1, LSTM2]) # Get state vector.default is axis = -1\n","    x = RepeatVector(6)(x)  # 6 hour ahead\n","\n","    # LSTM decoder\n","    x = LSTM(512, return_sequences=True)(x)\n","\n","    # final fully-connected layer for final result\n","    for dim in dim_dense:\n","      x = TimeDistributed(Dense(dim, activation='relu'))(x)\n","      x = TimeDistributed(Dropout(0.4))(x) # Some dropout for dense layers.\n","    main_out = TimeDistributed(Dense(1, activation='relu'))(x) \n","    main_out = Flatten()(main_out)\n","    #################################### nse mse ####################\n","    model = Model(inputs=[input_1, input_2], outputs=main_out)\n","    #model.compile(optimizer=RMSprop, loss='mse')\n","    model.compile(optimizer=RMSprop, loss=custom_regional_nse) \n","    model.summary()\n","\n","    # fit network\n","    history = model.fit([train1_x, train2_x], train_y, epochs=300, batch_size=128,\n","              validation_data=([val1_x, val2_x], val_y), callbacks=[reduce_lr,early_stopping], verbose=1)\n","\n","    from keras.models import load_model\n","    model.save('/content/drive/My Drive/master_thesis/_code/regional_model/pub_model/pub3_rm2_%s.h5' % num_ite) # last number indicate the num of folder\n","\n","    ######### modification ##########\n","    model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub_model/pub3_rm2_%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    #model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub_south_%s.h5' % num_ite)\n","    #nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub1.csv',index_col = 0)\n","    basins = nse_table.columns\n","    \n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","\n","    for i in range(len(test_sub_index)): # test on test basins\n","\n","        catchment = basins[test_sub_index[i]]\n","\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","\n","        dataset = testlist\n","        ed_nse = []\n","        ed_hour = []\n","        ed_volume = []\n","        ed_peak = []\n","        obs_peak = []\n","        dates = []\n","\n","        for j in range(len(dataset)):\n","            x1, x2,y = list(),list(), list()\n","            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","            date = dataset[j][:10]\n","            # MinMax for individual event\n","            n_input = 24\n","            n_output = 6\n","            sequence = min_max_scaler1.transform(data_1)\n","            temp_static = np.zeros(( n_input + n_output,14))\n","            for i in range(temp_static.shape[1]):\n","                temp_static[i,:] = pca_score_nor[catchment].values\n","            for i in range(len(sequence)):\n","                # find the end of this pattern\n","                end_ix1 = i*n_output + n_input # runoff\n","                end_ix2 = i*n_output + n_input + n_output # others\n","                end_y = end_ix1 + n_output\n","                # check if we are beyond the sequence\n","                if end_ix1 > len(sequence):\n","                    break\n","                if end_ix2 > len(sequence):\n","                    break\n","                if end_y > len(sequence):\n","                    break\n","                # gather input and output parts of the pattern\n","                temp_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","                # concatnate temp_static and seq_x2\n","\n","                seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","                x1.append(seq_x1)\n","                x2.append(seq_x2)\n","                y.append(seq_y)\n","\n","            x1 = np.array(x1)\n","            x2 = np.array(x2)\n","            y = np.array(y)\n","\n","            predictions = []\n","            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","            predict = model_pub.predict([x1,x2])\n","            predictions.append(predict)\n","            dates.append(date) #\n","            ed_peak.append(np.max(np.array(predictions).flatten()))\n","            obs_peak.append(np.max(np.array(y).flatten()))\n","            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n","\n","            # peak hour\n","            hour_pre = np.argmax(np.array(predictions).flatten())\n","            hour_obs = np.argmax(np.array(y).flatten())\n","            ed_hour.append((hour_pre - hour_obs))\n","            \n","            # peak discharge\n","            max_pre = np.max(np.array(predictions).flatten())\n","            max_obs = np.max(np.array(y).flatten())\n","            ed_volume.append((max_pre - max_obs)/max_obs)\n","\n","        temp = [dates, obs_peak, ed_peak, ed_hour, ed_volume] ###\n","        temp = np.array(temp)\n","        #output = pd.DataFrame([temp[0,:],temp[1,:],temp[2,:],temp[3,:]], index=['date', 'flow', 'hour','ratio'])\n","        #output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub2/%s_peak_pub3.csv' % catchment)\n","        nse_table[catchment] = np.median(ed_nse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V919jMkuWdhg"},"source":["nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/nse_pub3_rm2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qA0lSiBjGEq"},"source":["nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=4, shuffle = True,random_state = 1) # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in kf.split(basins):\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5O8GGLvAjJqi"},"source":["test_index_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROmFYwpGgrzx"},"source":["######################## first folder ##########################\n","### old ###\n","for k in range(len(train_index_list)):\n","#for k in range(6,8):\n","    num_ite = k + 1 # count the number of folder\n","    print (num_ite)\n","\n","    train_sub_index = train_index_list[k] # try the first folder\n","    train_file_all = []\n","    for i in range(len(train_sub_index)):\n","        catchment = basins[train_sub_index[i]]\n","        train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        trainlist = os.listdir(train_path)\n","        temp_name = []\n","        for i in range(len(trainlist)):\n","          temp = train_path + '/'+ trainlist[i]\n","          train_file_all.append(temp)\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","    for i in range(len(test_sub_index)):\n","        catchment = basins[test_sub_index[i]]\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        temp_name = []\n","        for i in range(len(testlist)):\n","          temp = test_path + '/'+ testlist[i]\n","          test_file_all.append(temp)\n","\n","\n","    # dataset split -- train_file_all\n","    # all data should use this scaler--fit the scaler using all data\n","    df2 = pd.DataFrame()\n","    for j in range(len(train_file_all)):\n","        data = pd.read_csv(train_file_all[j], index_col = 0)\n","        df2 = pd.concat([df2,data],axis=0) \n","    temp = df2.values\n","    min_max_scaler1 = MinMaxScaler() # default for each column\n","    min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","    # input for training model\n","    train1_x, train2_x, train_y, train_static = split_pub(pca_score_nor, train_file_all, 24, 6,min_max_scaler1,shuffle =True)\n","    #train1_x, train2_x, train_y = split_multi_sequence_keras(train_val, 24, 6,min_max_scaler1,shuffle =True)\n","\n","    # input for validation model -- using test\n","    val1_x, val2_x, val_y, val_static = split_pub(pca_score_nor, test_file_all, 24, 6, min_max_scaler1,shuffle =True)\n","    #val1_x, val2_x, val_y = split_multi_sequence_keras(test, 24, 6, min_max_scaler1,shuffle =True)\n","\n","    val1_x = val1_x.reshape((val1_x.shape[0],val1_x.shape[1],1)) \n","    train1_x = train1_x.reshape((train1_x.shape[0],train1_x.shape[1],1))\n","    ### only necessary when encoder layer is lstm\n","    #train_static = train_static.reshape((train_static.shape[0],1,train_static.shape[1]))\n","    #val_static = val_static.reshape((val_static.shape[0],1,val_static.shape[1]))\n","\n","    # define customer optimizer\n","    RMSprop=keras.optimizers.RMSprop(lr=0.0001)\n","\n","    # define special callbacks\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n","    ######## early longer ########\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=2)\n","    # early stop 50\n","    # design network\n","    dim_dense=[512, 256, 256, 128, 64]\n","\n","    # input of runoff observation, LSTM encoder\n","    input_1 = Input(shape=(train1_x.shape[1],train1_x.shape[2]), name='LSTM1_input') # shape should be 72*1 for runoff observation\n","    LSTM1 = LSTM(256, return_sequences=False)(input_1)\n","\n","    # input of runoff observation and forecast, LSTM encoder\n","    input_2 = Input(shape=(train2_x.shape[1],train2_x.shape[2]), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for n stations (if there is no upstream station, n=1)\n","    LSTM2 = LSTM(256, return_sequences=False)(input_2)\n","\n","    # input of other non-timeseries data, such as daily or monthly data.\n","    input_static = Input(shape=(14,), name='static_input') # shape = 14 -- 14 static variables -- 2D in this case with only Input or Dense layer (14,)/3D for LSTM layer (1,14)\n","    Layer3 = Dense(16, activation='relu')(input_static)\n","    #LSTM_static = LSTM(256, return_sequences=False)(input_static)\n","\n","\n","    # connect all data\n","    x = Concatenate()([Layer3, LSTM1, LSTM2]) # Get state vector.default is axis = -1\n","    x = RepeatVector(6)(x)  # 6 hour ahead\n","\n","    # LSTM decoder\n","    x = LSTM(512, return_sequences=True)(x)\n","\n","    # final fully-connected layer for final result\n","    for dim in dim_dense:\n","      x = TimeDistributed(Dense(dim, activation='relu'))(x)\n","      x = TimeDistributed(Dropout(0.4))(x) # Some dropout for dense layers.\n","    main_out = TimeDistributed(Dense(1, activation='relu'))(x) \n","    main_out = Flatten()(main_out)\n","    #################################### nse mse ####################\n","    model = Model(inputs=[input_static, input_1, input_2], outputs=main_out)\n","    #model.compile(optimizer=RMSprop, loss='mse')\n","    model.compile(optimizer=RMSprop, loss=custom_regional_nse) \n","    model.summary()\n","\n","    # fit network\n","    history = model.fit([train_static, train1_x, train2_x], train_y, epochs=300, batch_size=128,\n","              validation_data=([val_static, val1_x, val2_x], val_y), callbacks=[reduce_lr,early_stopping], verbose=1)\n","\n","    from keras.models import load_model\n","    model.save('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k4%s.h5' % num_ite) # last number indicate the num of folder\n","\n","    ######### modification ##########\n","    model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k4%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    #model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub_south_%s.h5' % num_ite)\n","    #nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub1.csv',index_col = 0)\n","    basins = nse_table.columns\n","    \n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","\n","    for i in range(len(test_sub_index)): # test on test basins\n","\n","        catchment = basins[test_sub_index[i]]\n","\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","\n","        dataset = testlist\n","        ed_nse = []\n","        ed_hour = []\n","        ed_volume = []\n","        ed_peak = []\n","        dates = []\n","\n","        for j in range(len(dataset)):\n","            x1, x2,y, seq_static = list(),list(), list(), list()\n","            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","            date = dataset[j][:10]\n","            # MinMax for individual event\n","            n_input = 24\n","            n_output = 6\n","            sequence = min_max_scaler1.transform(data_1)\n","            temp_static = np.zeros((1,14))\n","            temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n","            for i in range(len(sequence)):\n","                # find the end of this pattern\n","                end_ix1 = i*n_output + n_input # runoff\n","                end_ix2 = i*n_output + n_input + n_output # others\n","                end_y = end_ix1 + n_output\n","                # check if we are beyond the sequence\n","                if end_ix1 > len(sequence):\n","                    break\n","                if end_ix2 > len(sequence):\n","                    break\n","                if end_y > len(sequence):\n","                    break\n","                # gather input and output parts of the pattern\n","                seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","                # concatnate temp_static and seq_x2\n","\n","                #seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","                x1.append(seq_x1)\n","                x2.append(seq_x2)\n","                y.append(seq_y)\n","                seq_static.append(list(temp_static))\n","\n","            x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","            predictions = []\n","            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","            #seq_static = seq_static.reshape((seq_static.shape[0],1,seq_static.shape[1]))\n","            predict = model_pub.predict([seq_static, x1,x2]) # need be replaced\n","            predictions.append(predict)\n","            dates.append(date) #\n","            ed_peak.append(np.max(np.array(predictions).flatten()))\n","            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n","\n","            # peak hour\n","            hour_pre = np.argmax(np.array(predictions).flatten())\n","            hour_obs = np.argmax(np.array(y).flatten())\n","            ed_hour.append((hour_pre - hour_obs))\n","            \n","            # peak discharge\n","            max_pre = np.max(np.array(predictions).flatten())\n","            max_obs = np.max(np.array(y).flatten())\n","            ed_volume.append((max_pre - max_obs)/max_obs)\n","\n","        #temp = [dates, obs_peak, ed_peak, ed_hour, ed_volume] ###\n","        #temp = np.array(temp)\n","        #output = pd.DataFrame([temp[0,:],temp[1,:],temp[2,:],temp[3,:]], index=['date', 'flow', 'hour','ratio'])\n","        #output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub2/%s_peak_pub3.csv' % catchment)\n","        nse_table[catchment] = np.median(ed_nse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mDVJ-kbzZ49W"},"source":["nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/nse_pub3_axisk4.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZmMMt1hkFJh"},"source":["nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=3, shuffle = True,random_state = 1) # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in kf.split(basins):\n","#for test_index, train_index in kf.split(basins): # reverse so we have 12 for train 24 for test\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWLMwW8wkFmb"},"source":["######################## first folder ##########################\n","### old ###\n","for k in range(len(train_index_list)):\n","#for k in range(6,8):\n","    num_ite = k + 1 # count the number of folder\n","    print (num_ite)\n","\n","    train_sub_index = train_index_list[k] # try the first folder\n","    train_file_all = []\n","    for i in range(len(train_sub_index)):\n","        catchment = basins[train_sub_index[i]]\n","        train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        trainlist = os.listdir(train_path)\n","        temp_name = []\n","        for i in range(len(trainlist)):\n","          temp = train_path + '/'+ trainlist[i]\n","          train_file_all.append(temp)\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","    for i in range(len(test_sub_index)):\n","        catchment = basins[test_sub_index[i]]\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        temp_name = []\n","        for i in range(len(testlist)):\n","          temp = test_path + '/'+ testlist[i]\n","          test_file_all.append(temp)\n","\n","\n","    # dataset split -- train_file_all\n","    # all data should use this scaler--fit the scaler using all data\n","    df2 = pd.DataFrame()\n","    for j in range(len(train_file_all)):\n","        data = pd.read_csv(train_file_all[j], index_col = 0)\n","        df2 = pd.concat([df2,data],axis=0) \n","    temp = df2.values\n","    min_max_scaler1 = MinMaxScaler() # default for each column\n","    min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","    # input for training model\n","    train1_x, train2_x, train_y, train_static = split_pub(pca_score_nor, train_file_all, 24, 6,min_max_scaler1,shuffle =True)\n","    #train1_x, train2_x, train_y = split_multi_sequence_keras(train_val, 24, 6,min_max_scaler1,shuffle =True)\n","\n","    # input for validation model -- using test\n","    val1_x, val2_x, val_y, val_static = split_pub(pca_score_nor, test_file_all, 24, 6, min_max_scaler1,shuffle =True)\n","    #val1_x, val2_x, val_y = split_multi_sequence_keras(test, 24, 6, min_max_scaler1,shuffle =True)\n","\n","    val1_x = val1_x.reshape((val1_x.shape[0],val1_x.shape[1],1)) \n","    train1_x = train1_x.reshape((train1_x.shape[0],train1_x.shape[1],1))\n","    ### only necessary when encoder layer is lstm\n","    #train_static = train_static.reshape((train_static.shape[0],1,train_static.shape[1]))\n","    #val_static = val_static.reshape((val_static.shape[0],1,val_static.shape[1]))\n","\n","    # define customer optimizer\n","    RMSprop=keras.optimizers.RMSprop(lr=0.0001)\n","\n","    # define special callbacks\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n","    ######## early longer ########\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=2)\n","    # early stop 50\n","    # design network\n","    dim_dense=[512, 256, 256, 128, 64]\n","\n","    # input of runoff observation, LSTM encoder\n","    input_1 = Input(shape=(train1_x.shape[1],train1_x.shape[2]), name='LSTM1_input') # shape should be 72*1 for runoff observation\n","    LSTM1 = LSTM(256, return_sequences=False)(input_1)\n","\n","    # input of runoff observation and forecast, LSTM encoder\n","    input_2 = Input(shape=(train2_x.shape[1],train2_x.shape[2]), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for n stations (if there is no upstream station, n=1)\n","    LSTM2 = LSTM(256, return_sequences=False)(input_2)\n","\n","    # input of other non-timeseries data, such as daily or monthly data.\n","    input_static = Input(shape=(14,), name='static_input') # shape = 14 -- 14 static variables -- 2D in this case with only Input or Dense layer (14,)/3D for LSTM layer (1,14)\n","    Layer3 = Dense(16, activation='relu')(input_static)\n","    #LSTM_static = LSTM(256, return_sequences=False)(input_static)\n","\n","\n","    # connect all data\n","    x = Concatenate()([Layer3, LSTM1, LSTM2]) # Get state vector.default is axis = -1\n","    x = RepeatVector(6)(x)  # 6 hour ahead\n","\n","    # LSTM decoder\n","    x = LSTM(512, return_sequences=True)(x)\n","\n","    # final fully-connected layer for final result\n","    for dim in dim_dense:\n","      x = TimeDistributed(Dense(dim, activation='relu'))(x)\n","      x = TimeDistributed(Dropout(0.4))(x) # Some dropout for dense layers.\n","    main_out = TimeDistributed(Dense(1, activation='relu'))(x) \n","    main_out = Flatten()(main_out)\n","    #################################### nse mse ####################\n","    model = Model(inputs=[input_static, input_1, input_2], outputs=main_out)\n","    #model.compile(optimizer=RMSprop, loss='mse')\n","    model.compile(optimizer=RMSprop, loss=custom_regional_nse) \n","    model.summary()\n","\n","    # fit network\n","    history = model.fit([train_static, train1_x, train2_x], train_y, epochs=300, batch_size=128,\n","              validation_data=([val_static, val1_x, val2_x], val_y), callbacks=[reduce_lr,early_stopping], verbose=1)\n","\n","    from keras.models import load_model\n","    model.save('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k32%s.h5' % num_ite) # last number indicate the num of folder\n","    #model.save('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k3%s.h5' % num_ite) # last number indicate the num of folder\n","\n","    ######### modification ##########\n","    #model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k3%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k32%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    #model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub_south_%s.h5' % num_ite)\n","    #nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub1.csv',index_col = 0)\n","    basins = nse_table.columns\n","    \n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","\n","    for i in range(len(test_sub_index)): # test on test basins\n","\n","        catchment = basins[test_sub_index[i]]\n","\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","\n","        dataset = testlist\n","        ed_nse = []\n","        ed_hour = []\n","        ed_volume = []\n","        ed_peak = []\n","        dates = []\n","\n","        for j in range(len(dataset)):\n","            x1, x2,y, seq_static = list(),list(), list(), list()\n","            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","            date = dataset[j][:10]\n","            # MinMax for individual event\n","            n_input = 24\n","            n_output = 6\n","            sequence = min_max_scaler1.transform(data_1)\n","            temp_static = np.zeros((1,14))\n","            temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n","            for i in range(len(sequence)):\n","                # find the end of this pattern\n","                end_ix1 = i*n_output + n_input # runoff\n","                end_ix2 = i*n_output + n_input + n_output # others\n","                end_y = end_ix1 + n_output\n","                # check if we are beyond the sequence\n","                if end_ix1 > len(sequence):\n","                    break\n","                if end_ix2 > len(sequence):\n","                    break\n","                if end_y > len(sequence):\n","                    break\n","                # gather input and output parts of the pattern\n","                seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","                # concatnate temp_static and seq_x2\n","\n","                #seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","                x1.append(seq_x1)\n","                x2.append(seq_x2)\n","                y.append(seq_y)\n","                seq_static.append(list(temp_static))\n","\n","            x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","            predictions = []\n","            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","            #seq_static = seq_static.reshape((seq_static.shape[0],1,seq_static.shape[1]))\n","            predict = model_pub.predict([seq_static, x1,x2]) # need be replaced\n","            predictions.append(predict)\n","            dates.append(date) #\n","            ed_peak.append(np.max(np.array(predictions).flatten()))\n","            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n","\n","            # peak hour\n","            hour_pre = np.argmax(np.array(predictions).flatten())\n","            hour_obs = np.argmax(np.array(y).flatten())\n","            ed_hour.append((hour_pre - hour_obs))\n","            \n","            # peak discharge\n","            max_pre = np.max(np.array(predictions).flatten())\n","            max_obs = np.max(np.array(y).flatten())\n","            ed_volume.append((max_pre - max_obs)/max_obs)\n","\n","        #temp = [dates, obs_peak, ed_peak, ed_hour, ed_volume] ###\n","        #temp = np.array(temp)\n","        #output = pd.DataFrame([temp[0,:],temp[1,:],temp[2,:],temp[3,:]], index=['date', 'flow', 'hour','ratio'])\n","        #output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub2/%s_peak_pub3.csv' % catchment)\n","        nse_table[catchment] = np.median(ed_nse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aslSnH1xkQbj"},"source":["nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/nse_pub3_axisk33.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Sg39bSzkjfM"},"source":["nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=2, shuffle = True,random_state = 1) # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in kf.split(basins):\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FV0qhEUukjtj"},"source":["######################## first folder ##########################\n","### old ###\n","for k in range(len(train_index_list)):\n","#for k in range(6,8):\n","    num_ite = k + 1 # count the number of folder\n","    print (num_ite)\n","\n","    train_sub_index = train_index_list[k] # try the first folder\n","    train_file_all = []\n","    for i in range(len(train_sub_index)):\n","        catchment = basins[train_sub_index[i]]\n","        train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        trainlist = os.listdir(train_path)\n","        temp_name = []\n","        for i in range(len(trainlist)):\n","          temp = train_path + '/'+ trainlist[i]\n","          train_file_all.append(temp)\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","    for i in range(len(test_sub_index)):\n","        catchment = basins[test_sub_index[i]]\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        temp_name = []\n","        for i in range(len(testlist)):\n","          temp = test_path + '/'+ testlist[i]\n","          test_file_all.append(temp)\n","\n","\n","    # dataset split -- train_file_all\n","    # all data should use this scaler--fit the scaler using all data\n","    df2 = pd.DataFrame()\n","    for j in range(len(train_file_all)):\n","        data = pd.read_csv(train_file_all[j], index_col = 0)\n","        df2 = pd.concat([df2,data],axis=0) \n","    temp = df2.values\n","    min_max_scaler1 = MinMaxScaler() # default for each column\n","    min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","    # input for training model\n","    train1_x, train2_x, train_y, train_static = split_pub(pca_score_nor, train_file_all, 24, 6,min_max_scaler1,shuffle =True)\n","    #train1_x, train2_x, train_y = split_multi_sequence_keras(train_val, 24, 6,min_max_scaler1,shuffle =True)\n","\n","    # input for validation model -- using test\n","    val1_x, val2_x, val_y, val_static = split_pub(pca_score_nor, test_file_all, 24, 6, min_max_scaler1,shuffle =True)\n","    #val1_x, val2_x, val_y = split_multi_sequence_keras(test, 24, 6, min_max_scaler1,shuffle =True)\n","\n","    val1_x = val1_x.reshape((val1_x.shape[0],val1_x.shape[1],1)) \n","    train1_x = train1_x.reshape((train1_x.shape[0],train1_x.shape[1],1))\n","    ### only necessary when encoder layer is lstm\n","    #train_static = train_static.reshape((train_static.shape[0],1,train_static.shape[1]))\n","    #val_static = val_static.reshape((val_static.shape[0],1,val_static.shape[1]))\n","\n","    # define customer optimizer\n","    RMSprop=keras.optimizers.RMSprop(lr=0.0001)\n","\n","    # define special callbacks\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n","    ######## early longer ########\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=2)\n","    # early stop 50\n","    # design network\n","    dim_dense=[512, 256, 256, 128, 64]\n","\n","    # input of runoff observation, LSTM encoder\n","    input_1 = Input(shape=(train1_x.shape[1],train1_x.shape[2]), name='LSTM1_input') # shape should be 72*1 for runoff observation\n","    LSTM1 = LSTM(256, return_sequences=False)(input_1)\n","\n","    # input of runoff observation and forecast, LSTM encoder\n","    input_2 = Input(shape=(train2_x.shape[1],train2_x.shape[2]), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for n stations (if there is no upstream station, n=1)\n","    LSTM2 = LSTM(256, return_sequences=False)(input_2)\n","\n","    # input of other non-timeseries data, such as daily or monthly data.\n","    input_static = Input(shape=(14,), name='static_input') # shape = 14 -- 14 static variables -- 2D in this case with only Input or Dense layer (14,)/3D for LSTM layer (1,14)\n","    Layer3 = Dense(16, activation='relu')(input_static)\n","    #LSTM_static = LSTM(256, return_sequences=False)(input_static)\n","\n","\n","    # connect all data\n","    x = Concatenate()([Layer3, LSTM1, LSTM2]) # Get state vector.default is axis = -1\n","    x = RepeatVector(6)(x)  # 6 hour ahead\n","\n","    # LSTM decoder\n","    x = LSTM(512, return_sequences=True)(x)\n","\n","    # final fully-connected layer for final result\n","    for dim in dim_dense:\n","      x = TimeDistributed(Dense(dim, activation='relu'))(x)\n","      x = TimeDistributed(Dropout(0.4))(x) # Some dropout for dense layers.\n","    main_out = TimeDistributed(Dense(1, activation='relu'))(x) \n","    main_out = Flatten()(main_out)\n","    #################################### nse mse ####################\n","    model = Model(inputs=[input_static, input_1, input_2], outputs=main_out)\n","    #model.compile(optimizer=RMSprop, loss='mse')\n","    model.compile(optimizer=RMSprop, loss=custom_regional_nse) \n","    model.summary()\n","\n","    # fit network\n","    history = model.fit([train_static, train1_x, train2_x], train_y, epochs=300, batch_size=128,\n","              validation_data=([val_static, val1_x, val2_x], val_y), callbacks=[reduce_lr,early_stopping], verbose=1)\n","\n","    from keras.models import load_model\n","    model.save('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k2%s.h5' % num_ite) # last number indicate the num of folder\n","\n","    ######### modification ##########\n","    model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k2%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    #model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub_south_%s.h5' % num_ite)\n","    #nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub1.csv',index_col = 0)\n","    basins = nse_table.columns\n","    \n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","\n","    for i in range(len(test_sub_index)): # test on test basins\n","\n","        catchment = basins[test_sub_index[i]]\n","\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","\n","        dataset = testlist\n","        ed_nse = []\n","        ed_hour = []\n","        ed_volume = []\n","        ed_peak = []\n","        dates = []\n","\n","        for j in range(len(dataset)):\n","            x1, x2,y, seq_static = list(),list(), list(), list()\n","            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","            date = dataset[j][:10]\n","            # MinMax for individual event\n","            n_input = 24\n","            n_output = 6\n","            sequence = min_max_scaler1.transform(data_1)\n","            temp_static = np.zeros((1,14))\n","            temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n","            for i in range(len(sequence)):\n","                # find the end of this pattern\n","                end_ix1 = i*n_output + n_input # runoff\n","                end_ix2 = i*n_output + n_input + n_output # others\n","                end_y = end_ix1 + n_output\n","                # check if we are beyond the sequence\n","                if end_ix1 > len(sequence):\n","                    break\n","                if end_ix2 > len(sequence):\n","                    break\n","                if end_y > len(sequence):\n","                    break\n","                # gather input and output parts of the pattern\n","                seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","                # concatnate temp_static and seq_x2\n","\n","                #seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","                x1.append(seq_x1)\n","                x2.append(seq_x2)\n","                y.append(seq_y)\n","                seq_static.append(list(temp_static))\n","\n","            x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","            predictions = []\n","            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","            #seq_static = seq_static.reshape((seq_static.shape[0],1,seq_static.shape[1]))\n","            predict = model_pub.predict([seq_static, x1,x2]) # need be replaced\n","            predictions.append(predict)\n","            dates.append(date) #\n","            ed_peak.append(np.max(np.array(predictions).flatten()))\n","            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n","\n","            # peak hour\n","            hour_pre = np.argmax(np.array(predictions).flatten())\n","            hour_obs = np.argmax(np.array(y).flatten())\n","            ed_hour.append((hour_pre - hour_obs))\n","            \n","            # peak discharge\n","            max_pre = np.max(np.array(predictions).flatten())\n","            max_obs = np.max(np.array(y).flatten())\n","            ed_volume.append((max_pre - max_obs)/max_obs)\n","\n","        #temp = [dates, obs_peak, ed_peak, ed_hour, ed_volume] ###\n","        #temp = np.array(temp)\n","        #output = pd.DataFrame([temp[0,:],temp[1,:],temp[2,:],temp[3,:]], index=['date', 'flow', 'hour','ratio'])\n","        #output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub2/%s_peak_pub3.csv' % catchment)\n","        nse_table[catchment] = np.median(ed_nse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sl534fGUkn-A"},"source":["nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/nse_pub3_axisk23.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbebQgoYyD4H"},"source":["nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub_all.csv',index_col = 0)\n","basins = nse_table.columns\n","\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=35, shuffle = True,random_state = 1) # random_state to fix the order of shuffle\n","train_index_list = []\n","test_index_list = []\n","# all the train and test will be done under this for loop\n","for train_index, test_index in kf.split(basins):\n","    #print('X_train:%s ' % basins[train_index])\n","    train_index_list.append(train_index)\n","    test_index_list.append(test_index)\n","\n","\n","# add static variable to each time step -- pre-normalization not minmax -- negative value here for PCA scores\n","# read static\n","pca_score = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/pca_score.csv',index_col = 0)\n","# normalization -1 - 1 by each PC\n","temp2 = pca_score.values\n","temp_nor = preprocessing.normalize(temp2, norm='l2',axis = 1) # default for each column axis= 1\n","pca_score_nor = pd.DataFrame(temp_nor, index=[pca_score.index], columns=pca_score.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YKzHVHSgyFoc"},"source":["######################## first folder ##########################\n","### old ###\n","for k in range(len(train_index_list)):\n","#for k in range(6,8):\n","    num_ite = k + 1 # count the number of folder\n","    print (num_ite)\n","\n","    train_sub_index = train_index_list[k] # try the first folder\n","    train_file_all = []\n","    for i in range(len(train_sub_index)):\n","        catchment = basins[train_sub_index[i]]\n","        train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        trainlist = os.listdir(train_path)\n","        temp_name = []\n","        for i in range(len(trainlist)):\n","          temp = train_path + '/'+ trainlist[i]\n","          train_file_all.append(temp)\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","    for i in range(len(test_sub_index)):\n","        catchment = basins[test_sub_index[i]]\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        temp_name = []\n","        for i in range(len(testlist)):\n","          temp = test_path + '/'+ testlist[i]\n","          test_file_all.append(temp)\n","\n","\n","    # dataset split -- train_file_all\n","    # all data should use this scaler--fit the scaler using all data\n","    df2 = pd.DataFrame()\n","    for j in range(len(train_file_all)):\n","        data = pd.read_csv(train_file_all[j], index_col = 0)\n","        df2 = pd.concat([df2,data],axis=0) \n","    temp = df2.values\n","    min_max_scaler1 = MinMaxScaler() # default for each column\n","    min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","    # input for training model\n","    train1_x, train2_x, train_y, train_static = split_pub(pca_score_nor, train_file_all, 24, 6,min_max_scaler1,shuffle =True)\n","    #train1_x, train2_x, train_y = split_multi_sequence_keras(train_val, 24, 6,min_max_scaler1,shuffle =True)\n","\n","    # input for validation model -- using test\n","    val1_x, val2_x, val_y, val_static = split_pub(pca_score_nor, test_file_all, 24, 6, min_max_scaler1,shuffle =True)\n","    #val1_x, val2_x, val_y = split_multi_sequence_keras(test, 24, 6, min_max_scaler1,shuffle =True)\n","\n","    val1_x = val1_x.reshape((val1_x.shape[0],val1_x.shape[1],1)) \n","    train1_x = train1_x.reshape((train1_x.shape[0],train1_x.shape[1],1))\n","    ### only necessary when encoder layer is lstm\n","    #train_static = train_static.reshape((train_static.shape[0],1,train_static.shape[1]))\n","    #val_static = val_static.reshape((val_static.shape[0],1,val_static.shape[1]))\n","\n","    # define customer optimizer\n","    RMSprop=keras.optimizers.RMSprop(lr=0.0001)\n","\n","    # define special callbacks\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\n","    ######## early longer ########\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=2)\n","    # early stop 50\n","    # design network\n","    dim_dense=[512, 256, 256, 128, 64]\n","\n","    # input of runoff observation, LSTM encoder\n","    input_1 = Input(shape=(train1_x.shape[1],train1_x.shape[2]), name='LSTM1_input') # shape should be 72*1 for runoff observation\n","    LSTM1 = LSTM(256, return_sequences=False)(input_1)\n","\n","    # input of runoff observation and forecast, LSTM encoder\n","    input_2 = Input(shape=(train2_x.shape[1],train2_x.shape[2]), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for n stations (if there is no upstream station, n=1)\n","    LSTM2 = LSTM(256, return_sequences=False)(input_2)\n","\n","    # input of other non-timeseries data, such as daily or monthly data.\n","    input_static = Input(shape=(14,), name='static_input') # shape = 14 -- 14 static variables -- 2D in this case with only Input or Dense layer (14,)/3D for LSTM layer (1,14)\n","    Layer3 = Dense(16, activation='relu')(input_static)\n","    #LSTM_static = LSTM(256, return_sequences=False)(input_static)\n","\n","\n","    # connect all data\n","    x = Concatenate()([Layer3, LSTM1, LSTM2]) # Get state vector.default is axis = -1\n","    x = RepeatVector(6)(x)  # 6 hour ahead\n","\n","    # LSTM decoder\n","    x = LSTM(512, return_sequences=True)(x)\n","\n","    # final fully-connected layer for final result\n","    for dim in dim_dense:\n","      x = TimeDistributed(Dense(dim, activation='relu'))(x)\n","      x = TimeDistributed(Dropout(0.4))(x) # Some dropout for dense layers.\n","    main_out = TimeDistributed(Dense(1, activation='relu'))(x) \n","    main_out = Flatten()(main_out)\n","    #################################### nse mse ####################\n","    model = Model(inputs=[input_static, input_1, input_2], outputs=main_out)\n","    #model.compile(optimizer=RMSprop, loss='mse')\n","    model.compile(optimizer=RMSprop, loss=custom_regional_nse) \n","    model.summary()\n","\n","    # fit network\n","    history = model.fit([train_static, train1_x, train2_x], train_y, epochs=300, batch_size=128,\n","              validation_data=([val_static, val1_x, val2_x], val_y), callbacks=[reduce_lr,early_stopping], verbose=1)\n","\n","    from keras.models import load_model\n","    model.save('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k35%s.h5' % num_ite) # last number indicate the num of folder\n","\n","    ######### modification ##########\n","    model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_axis1_k35%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    #model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub_south_%s.h5' % num_ite)\n","    #nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub1.csv',index_col = 0)\n","    basins = nse_table.columns\n","    \n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","\n","    for i in range(len(test_sub_index)): # test on test basins\n","\n","        catchment = basins[test_sub_index[i]]\n","\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","\n","        dataset = testlist\n","        ed_nse = []\n","        ed_hour = []\n","        ed_volume = []\n","        ed_peak = []\n","        dates = []\n","\n","        for j in range(len(dataset)):\n","            x1, x2,y, seq_static = list(),list(), list(), list()\n","            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","            date = dataset[j][:10]\n","            # MinMax for individual event\n","            n_input = 24\n","            n_output = 6\n","            sequence = min_max_scaler1.transform(data_1)\n","            temp_static = np.zeros((1,14))\n","            temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n","            for i in range(len(sequence)):\n","                # find the end of this pattern\n","                end_ix1 = i*n_output + n_input # runoff\n","                end_ix2 = i*n_output + n_input + n_output # others\n","                end_y = end_ix1 + n_output\n","                # check if we are beyond the sequence\n","                if end_ix1 > len(sequence):\n","                    break\n","                if end_ix2 > len(sequence):\n","                    break\n","                if end_y > len(sequence):\n","                    break\n","                # gather input and output parts of the pattern\n","                seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","                # concatnate temp_static and seq_x2\n","\n","                #seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","                x1.append(seq_x1)\n","                x2.append(seq_x2)\n","                y.append(seq_y)\n","                seq_static.append(list(temp_static))\n","\n","            x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","            predictions = []\n","            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","            #seq_static = seq_static.reshape((seq_static.shape[0],1,seq_static.shape[1]))\n","            predict = model_pub.predict([seq_static, x1,x2]) # need be replaced\n","            predictions.append(predict)\n","            dates.append(date) #\n","            ed_peak.append(np.max(np.array(predictions).flatten()))\n","            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n","\n","            # peak hour\n","            hour_pre = np.argmax(np.array(predictions).flatten())\n","            hour_obs = np.argmax(np.array(y).flatten())\n","            ed_hour.append((hour_pre - hour_obs))\n","            \n","            # peak discharge\n","            max_pre = np.max(np.array(predictions).flatten())\n","            max_obs = np.max(np.array(y).flatten())\n","            ed_volume.append((max_pre - max_obs)/max_obs)\n","\n","        #temp = [dates, obs_peak, ed_peak, ed_hour, ed_volume] ###\n","        #temp = np.array(temp)\n","        #output = pd.DataFrame([temp[0,:],temp[1,:],temp[2,:],temp[3,:]], index=['date', 'flow', 'hour','ratio'])\n","        #output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub2/%s_peak_pub3.csv' % catchment)\n","        nse_table[catchment] = np.median(ed_nse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJOB0eQNyL9o"},"source":["nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/nse_pub3_axisk35.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZp5gvi5GfmP"},"source":["######### evaluation ###########\n","for k in range(len(train_index_list)):\n","    \n","    num_ite = k + 1 # count the number of folder\n","    print (num_ite)\n","    train_sub_index = train_index_list[k] # try the first folder\n","    train_file_all = []\n","    for i in range(len(train_sub_index)):\n","        catchment = basins[train_sub_index[i]]\n","        train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        trainlist = os.listdir(train_path)\n","        trainlist = sorted(trainlist)\n","        temp_name = []\n","        for i in range(len(trainlist)):\n","          temp = train_path + '/'+ trainlist[i]\n","          train_file_all.append(temp)\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","    for i in range(len(test_sub_index)):\n","        catchment = basins[test_sub_index[i]]\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        testlist = sorted(testlist)\n","        temp_name = []\n","        for i in range(len(testlist)):\n","          temp = test_path + '/'+ testlist[i]\n","          test_file_all.append(temp)\n","\n","\n","    # dataset split -- train_file_all\n","    # all data should use this scaler--fit the scaler using all data\n","    df2 = pd.DataFrame()\n","    for j in range(len(train_file_all)):\n","        data = pd.read_csv(train_file_all[j], index_col = 0)\n","        df2 = pd.concat([df2,data],axis=0) \n","    temp = df2.values\n","    min_max_scaler1 = MinMaxScaler() # default for each column\n","    min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","\n","    \n","    model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_new_%s.h5' % num_ite, custom_objects={'custom_regional_nse': custom_regional_nse})\n","    #nse_table = pd.read_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/basin_name_pub1.csv',index_col = 0)\n","    basins = nse_table.columns\n","\n","    test_sub_index = test_index_list[k]\n","    test_file_all = []\n","\n","    for i in range(len(test_sub_index)): # test on test basins\n","\n","        catchment = basins[test_sub_index[i]]\n","\n","        test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","        testlist = os.listdir(test_path)\n","        testlist = sorted(testlist)\n","        dataset = testlist\n","        ed_nse = []\n","        ed_hour = []\n","        ed_volume = []\n","        dates = []\n","        ed_peak = []\n","        obs_peak =[]\n","\n","        for j in range(len(dataset)):\n","            x1, x2,y, seq_static = list(),list(), list(), list()\n","            data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","            data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","            date = dataset[j][:10]\n","            # MinMax for individual event\n","            n_input = 24\n","            n_output = 6\n","            sequence = min_max_scaler1.transform(data_1)\n","            temp_static = np.zeros((1,14))\n","            temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n","            for i in range(len(sequence)):\n","                # find the end of this pattern\n","                end_ix1 = i*n_output + n_input # runoff\n","                end_ix2 = i*n_output + n_input + n_output # others\n","                end_y = end_ix1 + n_output\n","                # check if we are beyond the sequence\n","                if end_ix1 > len(sequence):\n","                    break\n","                if end_ix2 > len(sequence):\n","                    break\n","                if end_y > len(sequence):\n","                    break\n","                # gather input and output parts of the pattern\n","                seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","                seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","                # concatnate temp_static and seq_x2\n","\n","                #seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","                x1.append(seq_x1)\n","                x2.append(seq_x2)\n","                y.append(seq_y)\n","                seq_static.append(list(temp_static))\n","\n","            x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","            predictions = []\n","            x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","            #seq_static = seq_static.reshape((seq_static.shape[0],1,seq_static.shape[1]))\n","            predict = model_pub.predict([seq_static, x1,x2]) # need be replaced\n","            predictions.append(predict)\n","            dates.append(date) #\n","            ed_nse.append(r2_score(np.array(y).flatten(), np.array(predictions).flatten()))\n","            obs_peak.append(np.max(np.array(y).flatten()))\n","            ed_peak.append(np.max(np.array(predictions).flatten()))\n","\n","            # peak hour\n","            hour_pre = np.argmax(np.array(predictions).flatten())\n","            hour_obs = np.argmax(np.array(y).flatten())\n","            ed_hour.append((hour_pre - hour_obs))\n","            \n","            # peak discharge\n","            max_pre = np.max(np.array(predictions).flatten())\n","            max_obs = np.max(np.array(y).flatten())\n","            ed_volume.append(np.abs(max_pre - max_obs)/max_obs)\n","\n","        temp = [dates, obs_peak, ed_peak, ed_hour, ed_volume] ###\n","        temp = np.array(temp)\n","        output = pd.DataFrame(temp.T, columns=['date','obs','flow', 'hour','ratio'])\n","        output.to_csv('/content/drive/My Drive/master_thesis/_code/regional_model/peak_pub/%s_peak_pub3.csv' % catchment)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7iDNfNJCV61"},"source":["train_sub_index = train_index_list[0] # try the first folder\n","train_file_all = []\n","for i in range(len(train_sub_index)):\n","    catchment = basins[train_sub_index[i]]\n","    train_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","    trainlist = os.listdir(train_path)\n","    trainlist = sorted(trainlist)\n","    temp_name = []\n","    for i in range(len(trainlist)):\n","      temp = train_path + '/'+ trainlist[i]\n","      train_file_all.append(temp)\n","\n","test_sub_index = test_index_list[0]\n","test_file_all = []\n","for i in range(len(test_sub_index)):\n","    catchment = basins[test_sub_index[i]]\n","    test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","    testlist = os.listdir(test_path)\n","    testlist = sorted(testlist)\n","    temp_name = []\n","    for i in range(len(testlist)):\n","      temp = test_path + '/'+ testlist[i]\n","      test_file_all.append(temp)\n","\n","\n","# dataset split -- train_file_all\n","# all data should use this scaler--fit the scaler using all data\n","df2 = pd.DataFrame()\n","for j in range(len(train_file_all)):\n","    data = pd.read_csv(train_file_all[j], index_col = 0)\n","    df2 = pd.concat([df2,data],axis=0) \n","temp = df2.values\n","min_max_scaler1 = MinMaxScaler() # default for each column\n","min_max_scaler1.fit(temp) # only fit--apply this to the sequence window for normalization\n","\n","model_pub = load_model('/content/drive/My Drive/master_thesis/_code/regional_model/pub3_new_1.h5' , custom_objects={'custom_regional_nse': custom_regional_nse})\n","catchment = 'Jiangxi'\n","\n","test_path = '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/model1/%s_events' % catchment\n","testlist = os.listdir(test_path)\n","testlist = sorted(testlist)\n","dataset = testlist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWdHqIY2EWeS"},"source":["testlist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmJaLa6aB-Iv"},"source":["\n","ed_nse = []\n","\n","\n","j = -9\n","x1, x2,y, seq_static = list(),list(), list(), list()\n","data_1 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0)\n","data_2 = pd.read_csv(test_path+'/'+dataset[j], index_col = 0) # not normalize y(target)\n","\n","# MinMax for individual event\n","n_input = 24\n","n_output = 6\n","sequence = min_max_scaler1.transform(data_1)\n","temp_static = np.zeros((1,14))\n","temp_static = pca_score_nor[catchment].values # each basin one static input only one step\n","for i in range(len(sequence)):\n","    # find the end of this pattern\n","    end_ix1 = i*n_output + n_input # runoff\n","    end_ix2 = i*n_output + n_input + n_output # others\n","    end_y = end_ix1 + n_output\n","    # check if we are beyond the sequence\n","    if end_ix1 > len(sequence):\n","        break\n","    if end_ix2 > len(sequence):\n","        break\n","    if end_y > len(sequence):\n","        break\n","    # gather input and output parts of the pattern\n","    seq_x2 = sequence[:,1:][i*n_output:end_ix2, :] # 3 rainfall, tmax and tmin -- add static to each time step\n","    seq_x1, seq_y = sequence[:,0][i*n_output:end_ix1], data_2.values[:,0][end_ix1:end_y] # 24 hours runoff and target runoff -- 0 to -1\n","    # concatnate temp_static and seq_x2\n","\n","    #seq_x2 = np.concatenate((temp_x2,temp_static), axis= 1) # axis= 0, row to row\n","    x1.append(seq_x1)\n","    x2.append(seq_x2)\n","    y.append(seq_y)\n","    seq_static.append(list(temp_static))\n","\n","x1, x2,y,seq_static = np.array(x1), np.array(x2), np.array(y), np.array(seq_static)\n","predictions = []\n","x1 = x1.reshape((x1.shape[0],x1.shape[1],1))\n","#seq_static = seq_static.reshape((seq_static.shape[0],1,seq_static.shape[1]))\n","predict = model_pub.predict([seq_static, x1,x2]) # need be replaced\n","predictions.append(predict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Qgl98C5DrBd","executionInfo":{"status":"ok","timestamp":1615479730257,"user_tz":-60,"elapsed":592,"user":{"displayName":"Yikui Zhang","photoUrl":"","userId":"13264182408663850937"}},"outputId":"436018ce-ba7e-4031-c3f0-625aeee20582"},"source":["r2_score(np.array(y).flatten(), np.array(predictions).flatten())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7432815294295352"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"z4zPbOMSDbMT"},"source":["pub3 = np.array(predictions).flatten()\n","\n","event_2=pd.DataFrame(pub3,columns= ['pub'])\n","event_2.to_csv(r'/content/drive/My Drive/master_thesis/results/image/local/2012-07-06pub.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJzZngulEYz2"},"source":["output = pd.DataFrame(temp.T, columns=['date','obs','flow', 'hour','ratio'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZX6bfE-ddSk","executionInfo":{"status":"ok","timestamp":1613154130112,"user_tz":-60,"elapsed":728,"user":{"displayName":"张一夔","photoUrl":"","userId":"13264182408663850937"}},"outputId":"87757235-701a-42ff-c12c-a07fe0489044"},"source":["temp[1,:].reshape((93,1)).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(93, 1)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"VOs_zBEqG-EJ"},"source":["nse_table.to_csv( '/content/drive/My Drive/master_thesis/_code/regional_model/regional_data/nse_pub_south.csv')"],"execution_count":null,"outputs":[]}]}